# dataset:
#   noisy_test_dataset_dir: /Users/vtokala/Documents/Research/di_nn/Dataset/noisy_testset_1f
#   noisy_training_dataset_dir: /Users/vtokala/Documents/Research/di_nn/Dataset/noisy_trainset_1f
#   noisy_validation_dataset_dir: /Users/vtokala/Documents/Research/di_nn/Dataset/noisy_valset_1f
#   target_test_dataset_dir: /Users/vtokala/Documents/Research/di_nn/Dataset/clean_testset_1f
#   target_training_dataset_dir: /Users/vtokala/Documents/Research/di_nn/Dataset/clean_trainset_1f
#   target_validation_dataset_dir: /Users/vtokala/Documents/Research/di_nn/Dataset/clean_valset_1f
# defaults:
# - model
# - training
# - dataset: speech_dataset
# model:
#   attention: true
#   ild_weight: 1
#   ipd_weight: 10
#   snr_loss_weight: 1
#   stoi_weight: 10
# training:
#   accelerator: cpu
#   batch_size: 20
#   learning_rate: 0.0001
#   n_epochs: 20
#   n_workers: 4
#   pin_memory: true
#   strategy: ddp_spawn
#   train_checkpoint_path: null
dataset:
  noisy_test_dataset_dir: /raid/R12K41024/BCCTN/Dataset/noisy_testset
  noisy_training_dataset_dir: /raid/R12K41024/BCCTN/Dataset/noisy_trainset
  noisy_validation_dataset_dir: /raid/R12K41024/BCCTN/Dataset/noisy_valset
  target_test_dataset_dir: /raid/R12K41024/BCCTN/Dataset/clean_testset
  target_training_dataset_dir: /raid/R12K41024/BCCTN/Dataset/clean_trainset
  target_validation_dataset_dir: /raid/R12K41024/BCCTN/Dataset/clean_valset
defaults:
- model
- training
- dataset: speech_dataset
model:
  attention: true
  binaural: true
  # Loss function weights as described in the paper (section 3.1)
  ild_weight: 1        # Weight for ILD loss (γ)
  ipd_weight: 10       # Weight for IPD loss (κ)
  snr_loss_weight: 1   # Weight for SNR loss (α)
  stoi_weight: 10      # Weight for STOI loss (β)
  # Model architecture parameters
  use_clstm: true
  use_cbn: true
  masking_mode: 'E'
  rnn_layers: 2
  rnn_units: 128
  win_len: 400         # 25ms at 16kHz
  win_inc: 100         # 6.25ms at 16kHz
  fft_len: 512
  win_type: 'hann'
  kernel_size: 5
  # Kernel numbers for encoder-decoder layers as per paper
  kernel_num: [16, 32, 64, 128, 256, 256]
  bidirectional: false
  embed_dim: 512       # Embedding dimension for transformer
  num_heads: 32        # Number of attention heads as mentioned in paper
training:
  accelerator: auto
  batch_size: 32
  learning_rate: 0.001  # Initial learning rate as per paper
  learning_rate_decay_steps: [3, 8]
  learning_rate_decay_values: 0.5
  n_epochs: 100         # Number of epochs as per paper
  n_workers: 4
  pin_memory: true
  strategy: auto
  train_checkpoint_path: null
  early_stopping:
    enabled: true
    key_to_monitor: validation_loss
    min_delta: 0.01
    patience_in_epochs: 3  # Early stopping after 3 epochs of no improvement